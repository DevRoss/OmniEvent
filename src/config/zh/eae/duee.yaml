seed: 42
do_train: True
do_predict: True

paradigm: sequence_labeling

task_name: EAE
golden_trigger: True
dataset_name: DuEE1.0
output_dir: output/DuEE1.0
type2id_path: ../data/DuEE1.0/label2id.json
role2id_path: ../data/DuEE1.0/role2id.json
train_file: ../data/DuEE1.0/duee_train.unified.json
validation_file: ../data/DuEE1.0/duee_dev.unified.json
test_file: ../data/DuEE1.0/duee_test2.unified.json
train_pred_file: output/DuEE1.0/ED/sequence_labeling/bert-base-chinese/train_preds.json
validation_pred_file: output/DuEE1.0/ED/sequence_labeling/bert-base-chinese/valid_preds.json
test_pred_file: output/DuEE1.0/ED/sequence_labeling/bert-base-chinese/test_preds.json
language: Chinese
test_exists_labels: False

return_token_type_ids: True
truncate_seq2seq_output: False

model_type: bert
model_name_or_path: bert-base-chinese
hidden_size: 768
aggregation: none

num_train_epochs: 1
max_seq_length: 200
max_out_length: 200
dataloader_num_workers: 2

generation_max_length: 128
generation_num_beams: 4
predict_with_generate: True
ignore_pad_token_for_loss: True

per_device_train_batch_size: 16
per_device_eval_batch_size: 8
gradient_accumulation_steps: 1
# eval_accumulation_steps: 4
learning_rate: 3.0e-5
weight_decay: 1.0e-5
warmup_ratio: 0.1
max_grad_norm: 1
optim: adamw_torch

load_best_model_at_end: True
metric_for_best_model: micro_f1
greater_is_better: True

logging_strategy: steps
logging_steps: 100
evaluation_strategy: epoch
eval_steps: 500
save_strategy: epoch

# split inference #
split_infer: True
split_infer_size: 5000

